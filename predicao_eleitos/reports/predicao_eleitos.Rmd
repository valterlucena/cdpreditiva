---
title: "Predição de Deputados Eleitos"
author: "Valter Lucena"
date: "20 de novembro de 2018"
output: 
  html_document:
    df_print: paged
    toc: yes
    toc_float: yes
  html_notebook:
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(here)
library(caret)
library(ROSE)
theme_set(theme_minimal())
```

# Introdução

Nesta análise iremos construir modelos de classificação para a predição da eleição dos candidatos à Câmara Federal de Deputados. Anteriormente, [nesta análise](https://rpubs.com/valterlucena/pred-votos), buscamos predizer o total de votos recebidos utilizando técnicas de regressão.

```{r}
train <- read.csv(here("data/train.csv")) 
train %>% names()
```

# Perguntas

## Há desbalanceamento das classes (isto é, uma classe tem muito mais instâncias que outra)? Em que proporção? Quais efeitos colaterais o desbalanceamento de classes pode causar no classificador? Como você poderia tratar isso?

```{r}
eleitos <- train %>% 
  filter(situacao == "eleito") %>% 
  count(situacao)

nao_eleitos <- train %>% 
  filter(situacao == "nao_eleito") %>% 
  count(situacao)

frame = rbind(eleitos, nao_eleitos)

frame %>% 
  mutate(proporcao = n / (train %>% nrow())) %>% 
  ggplot(aes(x = situacao,
             y = n,
             colour = situacao,
             label = paste(n,"(",round(proporcao * 100, 2),"%)"))) +
  geom_point(size = 2) +
  geom_segment(aes(x = situacao,
                   xend = situacao,
                   y = 0,
                   yend = n)) +
  geom_text(hjust = 1,
            vjust = -1) +
  guides(colour = FALSE) +
  labs(x = "Situação",
       y = "Quantidade de deputados") +
  theme() +
  coord_flip()
```

Como podemos observar no gráfico, existe uma grande diferença entre a quantidade de deputados eleitos e não eleitos na base de dados de treino, ou seja, há sim desbalanceamento entre as classes. Nestes dados, 86.54% são da classe `nao_eleito` e 13.46% são da classe `eleito`. Um efeito desse desbalanceamento é que as predições realizadas utilizando um modelo que foi treinado com esses dados podem ser enviesadas para a classe majoritária. A solução mais comum para este tipo de problema é realizar uma reamostragem nos dados utilizando *undersampling* ou *oversampling*.

* *Undersampling*: são retiradas aleatoriamente amostras da classe majoritária de forma a igualar a quantidade de observações desta com a classe minoritária. Uma desvantagem dessa abordagem é a perda de informação.
* *Oversampling*: as observações da classe minoritária são aleatoriamente duplicadas ou são geradas novas observações para se igualar a quantidade de observações da classe majoritária. Nesta abordagem não há risco de perda de informação, mas aumenta-se o risco de que ocorra um *overfitting*, uma vez que as mesmas amostras podem ser retiradas. Com isto, a capacidade de generalização do modelo seria prejudicada. 

## Treine: um modelo de regressão logística, uma árvore de decisão e um modelo de adaboost. Tune esses modelos usando validação cruzada e controle overfitting se necessário, considerando as particularidades de cada modelo. 

Vamos, inicialmente, retirar dos dados as variáveis de identificação de um deputado, e as categóricas de apenas um nível.

```{r}
train <- train %>% 
  select(-sequencial_candidato,
         -nome,
         -cargo,
         -uf,
         -ocupacao,
         -partido,
         -estado_civil,
         -grau,
         -ocupacao)
```

Para obtermos uma estimativa honesta do desempenho de cada modelo, utilizaremos ambas as abordagens para desbalanceamento de forma independente a cada fold da validação cruzada.

*Observação*: Os códigos para o treinamento dos modelos estão comentados para que não sejam reexecutados durante a geração do html deste relatório. 

### Modelo de Regressão Logística

```{r}
# reglogControl <- trainControl(method = "repeatedcv",
#                               sampling = "smote",
#                               number = 5,
#                               repeats = 5,
#                               verboseIter = TRUE,
#                               classProbs = TRUE)
# 
# regLog <- train(situacao ~ .,
#                 train,
#                 method = "regLogistic",
#                 trControl = reglogControl,
#                 preProcess = c('scale', 'center', 'nzv'))

# saveRDS(regLog, file = "regLog.rds")
regLog <- readRDS(file = "regLog.rds")
regLog
```

```{r}
plot(regLog)
```

### Modelo KNN (K-Nearest Neighbours)

```{r}
# knnGrid <- expand.grid(k = seq(1, 25, length=25))
# 
# knnControl <- trainControl(method = "repeatedcv",
#                            sampling = "smote",
#                            number = 5,
#                            repeats = 5,
#                            verboseIter = TRUE,
#                            classProbs = TRUE)
# knn <- train(situacao ~ .,
#              data = train,
#              method = "knn",
#              trControl = knnControl,
#              preProcess = c('scale', 'center', 'nzv'),
#              tuneGrid = knnGrid)
# 
# saveRDS(knn, file = 'knnModel.rds')
knn <- readRDS(file = 'knnModel.rds')
knn
```

```{r}
plot(knn)
```

### Árvore de Decisão

```{r}
# arvore decisao
# dtControl <- trainControl(method = "repeatedcv",
#                           number = 5,
#                           repeats = 5,
#                           sampling = "smote",
#                           verboseIter = TRUE,
#                           classProbs = TRUE)
# 
# dtree <- train(situacao ~ .,
#                data = train,
#                method = "rpart",
#                trControl = dtControl,
#                preProcess = c('scale', 'center', 'nzv'),
#                cp = 0.001)
# 
# saveRDS(dtree, 'dtree.rds')
dtree <- readRDS(file = "dtree.rds")
dtree
```

```{r}
library(rpart.plot)
rpart.plot(dtree$finalModel)
```

### Adaboost

```{r}
# adaboost
# adaGrid <- expand.grid(nIter = seq(1, 25, length = 25),
#                        method = c("Adaboost.M1", "Real adaboost"))
# 
# adaControl <- trainControl(method = "repeatedcv",
#                            sampling = "smote",
#                            number = 5,
#                            repeats = 5,
#                            verboseIter = TRUE,
#                            classProbs = TRUE)
# 
# adaboost <- train(situacao ~ .,
#                   train,
#                   method = "adaboost",
#                   trControl = adaControl,
#                   preProcess = c("scale", "center", "nzv"),
#                   tuneGrid = adaGrid)
# 
# saveRDS(adaboost, file = "adaboost.rds")
adaboost <- readRDS(file = "adaboost.rds")
adaboost
```

```{r}
plot(adaboost)
```

## Reporte precision, recall e f-measure no treino e validação. Há uma grande diferença de desempenho no treino/validação? Como você avalia os resultados? Justifique sua resposta. 

Todas essas medidas são calculadas em termos de Verdadeiros Positivos (TP), Verdadeiros Negativos (TN), Falsos Positivos (FP) e Falsos Negativos (FN), ou seja, na quantidade de observações onde um candidato previsto como eleito ou não eleito realmente se elegeu ou não se elegeu. Cada medida representa o seguinte:

* Precision: razão entre observações previstas como positivas corretamente (TP) e o total de observações previstas como positivas (TP + FP)

```{r}
precision <- function(TP, FP) {
  result <- TP / (TP + FP)
  return(result)
}
```

* Recall: proporção de observações previstas como positivas corretamente (TP) entre todas as observações da classe atual (TP + FN)

```{r}
recall <- function(TP,FN) {
  result <- TP / (TP + FN)
  return(result)
}
```

* F-measure: média ponderada entre *precision* e *recall*. Leva em consideração tanto os falso positivos como os falsos negativos

```{r}
fmeasure <- function(precision, recall) {
  result <- 2 * (precision * recall) / (precision + recall)
}
```

Pra o modelo de Regressão Logística, temos:

```{r}
matriz.regLog <- confusionMatrix(regLog)
regLog.precision <- precision(matriz.regLog$table[1], matriz.regLog$table[3])
regLog.recall <- recall(matriz.regLog$table[1], matriz.regLog$table[2])
regLog.fmeasure <- fmeasure(regLog.precision, regLog.recall)

regLog.precision
regLog.recall
regLog.fmeasure
```

Pra o modelo KNN, temos:

```{r}
matriz.knn <- confusionMatrix(knn)
knn.precision <- precision(matriz.knn$table[1], matriz.knn$table[3])
knn.recall <- recall(matriz.knn$table[1], matriz.knn$table[2])
knn.fmeasure <- fmeasure(knn.precision, knn.recall)

knn.precision
knn.recall
knn.fmeasure
```

Pra a árvore de decisão, temos:

```{r}
matriz.dtree <- confusionMatrix(dtree)
dtree.precision <- precision(matriz.dtree$table[1], matriz.dtree$table[3])
dtree.recall <- recall(matriz.dtree$table[1], matriz.dtree$table[2])
dtree.fmeasure <- fmeasure(dtree.precision, dtree.recall)

dtree.precision
dtree.recall
dtree.fmeasure
```

Para o modelo de Adaboost, temos:

```{r}
matriz.adaboost <- confusionMatrix(adaboost)
adaboost.precision <- precision(matriz.adaboost$table[1], matriz.adaboost$table[3])
adaboost.recall <- recall(matriz.adaboost$table[1], matriz.adaboost$table[2])
adaboost.fmeasure <- fmeasure(adaboost.precision, adaboost.recall)

adaboost.precision
adaboost.recall
adaboost.fmeasure
```

## Interprete as saídas dos modelos. Quais atributos parecem ser mais importantes de acordo com cada modelo?

```{r}
ggplot(varImp(dtree))
```

Para a árvore de decisão, as variáveis mais importantes foram `total_receita`, `total_despesa`, `recursos_de_pessoas_juridicas`, `quantidade_despesas` e `quantidade_fornecedores`. 

## Envie seus melhores modelos à competição do Kaggle. Faça pelo menos uma submissão. Sugestões para melhorar o modelo:

```{r}
# best.model.grid <- expand.grid(adaboost$bestTune)
# best.model <- train(situacao ~ .,
#                     train,
#                     method = "adaboost",
#                     trControl = trainControl(verboseIter = TRUE),
#                     tuneGrid = best.model.grid)
# saveRDS(best.model, file="bestModel.rds")
best.model <- readRDS(file="bestModel.rds")
best.model
```

Primeiramente vamos aplicar todas as modificações do modelo de treino no modelo de teste.

```{r}
test <- read.csv(here("data/test.csv"))
submission <- test %>% select(sequencial_candidato)
test <- test %>% 
  select(-sequencial_candidato,
         -nome,
         -cargo,
         -uf,
         -ocupacao,
         -partido,
         -estado_civil,
         -grau,
         -ocupacao)
predictions <- predict(best.model, test)
submission$situacao <- predictions
submission <- submission %>% 
  select(Id = sequencial_candidato,
         Predicted = situacao)
write.csv(x = submission,
          file = "sample_submission.csv",
          row.names = FALSE)
```

