---
title: "Predição de Votação de Deputados"
author: "Valter Lucena"
date: "29 de outubro de 2018"
output: 
  html_notebook:
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(here)
library(caret)
library(ggplot2)
options(max.print = .Machine$integer.max)
set.seed(37)
theme_set(theme_minimal())
```

# Introdução

Nesta análise iremos construir modelos preditivos de regressão para a predição de votação de candidatos à Câmara Federal de Deputados. Anteriormente, [nesta análise](http://rpubs.com/valterlucena/votacao_regressao), buscamos explicar essas votações através da regressão linear multivariada. 

Uma regressão com muitos coeficientes torna o modelo muito complexo, dificultando sua interpretação. Uma das consequências disso é o *overfitting*, que acontece quando o modelo se adpta ao ruído dos dados. Os métodos Ridge, Lasso e KNN são uma forma de eliminar esse problema, pois reduzem a complexidade do modelo, minimizando as chances do overfitting acontecer.

Nesta análise, utilizaremos estes métodos para prever a quantidade de votos que um candidato receberá.

Primeiramente, vamos importar nossos dados. Uma melhor descrição das variáveis pode ser encontrada, também, [aqui](http://rpubs.com/valterlucena/votacao_regressao).

```{r message=FALSE, warning=FALSE}
train <- read.csv(here("data/train.csv"))
```

# Perguntas

## Usando todas as variáveis disponíveis, tune (usando validação cruzada): (i) um modelo de regressão Ridge, (ii) um modelo de regressão Lasso e (iii) um modelo KNN. Para os modelos de regressão linear, o parâmetro a ser tunado é o lambda (penalização dos coeficientes) e o KNN o número de vizinhos.

Inicialmente, vamos retirar dos nossos dados as colunas que servem apenas para identificar um candidato, a variável `cargo`, que possui apenas um valor, e as variáveis categóricas de vários níveis.

```{r}
train <- train %>% 
  select(-sequencial_candidato,
         -nome,
         -cargo,
         -uf,
         -ocupacao)
```

Agora, vamos utilizar a validação cruzada.

```{r}
fitControl <- trainControl(method = "cv",
                           number = 10,
                           search = "random")
```

### (i) Modelo de Regressão Ridge.

```{r eval=FALSE}
lambdaGrid <- expand.grid(lambda = 10^seq(10, -2, length=100))

ridge <- train(votos ~ ., 
               data = train,
               method = "ridge",
               trControl = fitControl,
               preProcess = c('scale', 'center', 'nzv'),
               tuneGrid = lambdaGrid)
ridge
```


```{r}
# Ridge Regression 
# 
# 7476 samples
#   18 predictor
# 
# Pre-processing: scaled (27), centered (27), remove (28) 
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 6729, 6728, 6728, 6728, 6728, 6729, ... 
# Resampling results across tuning parameters:
# 
#   lambda        RMSE       Rsquared   MAE     
#   1.000000e-02   39363.13  0.3909655  16352.28
#   1.321941e-02   39418.52  0.3904506  16352.03
#   1.747528e-02   39481.00  0.3899094  16353.48
#   2.310130e-02   39551.14  0.3893622  16351.08
#   3.053856e-02   39630.26  0.3888290  16343.48
#   4.037017e-02   39720.98  0.3883255  16329.04
#   5.336699e-02   39827.77  0.3878588  16306.23
#   7.054802e-02   39957.68  0.3874252  16273.27
#   9.326033e-02   40121.33  0.3870072  16231.26
#   1.232847e-01   40334.31  0.3865745  16182.85
#   1.629751e-01   40619.38  0.3860860  16138.50
#   2.154435e-01   41009.58  0.3854951  16102.52
#   2.848036e-01   41553.04  0.3847583  16087.82
#   3.764936e-01   42319.36  0.3838443  16129.83
#   4.977024e-01   43407.39  0.3827425  16277.85
#   6.579332e-01   44952.44  0.3814671  16607.09
#   8.697490e-01   47128.56  0.3800559  17234.85
#   1.149757e+00   50139.31  0.3785649  18312.46
#   1.519911e+00   54190.61  0.3770588  19978.40
#   2.009233e+00   59445.92  0.3756018  22373.38
#   2.656088e+00   65974.28  0.3742477  25512.81
#   3.511192e+00   73709.52  0.3730346  29413.23
#   4.641589e+00   82436.68  0.3719822  33918.67
#   6.135907e+00   91812.86  0.3710938  38771.28
#   8.111308e+00  101418.33  0.3703609  43751.09
#   1.072267e+01  110824.06  0.3697674  48621.83
#   1.417474e+01  119656.47  0.3692939  53203.98
#   1.873817e+01  127642.52  0.3689206  57365.77
#   2.477076e+01  134626.63  0.3686290  61002.71
#   3.274549e+01  140562.10  0.3684029  64090.41
#   4.328761e+01  145486.55  0.3682286  66649.05
#   5.722368e+01  149492.16  0.3680948  68730.33
#   7.564633e+01  152698.63  0.3679924  70396.87
#   1.000000e+02  155232.84  0.3679144  71713.59
#   1.321941e+02  157215.67  0.3678549  72744.58
#   1.747528e+02  158754.95  0.3678098  73545.97
#   2.310130e+02  159942.64  0.3677754  74164.13
#   3.053856e+02  160854.75  0.3677494  74638.71
#   4.037017e+02  161552.70  0.3677297  75001.87
#   5.336699e+02  162085.31  0.3677147  75279.01
#   7.054802e+02  162490.89  0.3677034  75490.04
#   9.326033e+02  162799.25  0.3676948  75650.50
#   1.232847e+03  163033.40  0.3676884  75772.34
#   1.629751e+03  163211.03  0.3676834  75864.77
#   2.154435e+03  163345.71  0.3676797  75934.83
#   2.848036e+03  163447.75  0.3676769  75987.92
#   3.764936e+03  163525.04  0.3676748  76028.12
#   4.977024e+03  163583.56  0.3676732  76058.57
#   6.579332e+03  163627.86  0.3676720  76081.62
#   8.697490e+03  163661.39  0.3676710  76099.06
#   1.149757e+04  163686.76  0.3676703  76112.26
#   1.519911e+04  163705.97  0.3676698  76122.25
#   2.009233e+04  163720.49  0.3676694  76129.81
#   2.656088e+04  163731.49  0.3676691  76135.53
#   3.511192e+04  163739.80  0.3676689  76139.85
#   4.641589e+04  163746.09  0.3676687  76143.12
#   6.135907e+04  163750.85  0.3676686  76145.60
#   8.111308e+04  163754.46  0.3676685  76147.47
#   1.072267e+05  163757.18  0.3676684  76148.89
#   1.417474e+05  163759.24  0.3676684  76149.96
#   1.873817e+05  163760.80  0.3676683  76150.77
#   2.477076e+05  163761.98  0.3676683  76151.39
#   3.274549e+05  163762.87  0.3676683  76151.85
#   4.328761e+05  163763.54  0.3676682  76152.20
#   5.722368e+05  163764.06  0.3676682  76152.47
#   7.564633e+05  163764.44  0.3676682  76152.67
#   1.000000e+06  163764.73  0.3676682  76152.82
#   1.321941e+06  163764.95  0.3676682  76152.94
#   1.747528e+06  163765.12  0.3676682  76153.02
#   2.310130e+06  163765.25  0.3676682  76153.09
#   3.053856e+06  163765.34  0.3676682  76153.14
#   4.037017e+06  163765.42  0.3676682  76153.18
#   5.336699e+06  163765.47  0.3676682  76153.20
#   7.054802e+06  163765.51  0.3676682  76153.23
#   9.326033e+06  163765.54  0.3676682  76153.24
#   1.232847e+07  163765.57  0.3676682  76153.25
#   1.629751e+07  163765.59  0.3676682  76153.26
#   2.154435e+07  163765.60  0.3676682  76153.27
#   2.848036e+07  163765.61  0.3676682  76153.28
#   3.764936e+07  163765.62  0.3676682  76153.28
#   4.977024e+07  163765.62  0.3676682  76153.28
#   6.579332e+07  163765.63  0.3676682  76153.29
#   8.697490e+07  163765.63  0.3676682  76153.29
#   1.149757e+08  163765.63  0.3676682  76153.29
#   1.519911e+08  163765.63  0.3676682  76153.29
#   2.009233e+08  163765.64  0.3676682  76153.29
#   2.656088e+08  163765.64  0.3676682  76153.29
#   3.511192e+08  163765.64  0.3676682  76153.29
#   4.641589e+08  163765.64  0.3676682  76153.29
#   6.135907e+08  163765.64  0.3676682  76153.29
#   8.111308e+08  163765.64  0.3676682  76153.29
#   1.072267e+09  163765.64  0.3676682  76153.29
#   1.417474e+09  163765.64  0.3676682  76153.29
#   1.873817e+09  163765.64  0.3676682  76153.29
#   2.477076e+09  163765.64  0.3676682  76153.29
#   3.274549e+09  163765.64  0.3676682  76153.29
#   4.328761e+09  163765.64  0.3676682  76153.29
#   5.722368e+09  163765.64  0.3676682  76153.29
#   7.564633e+09  163765.64  0.3676682  76153.29
#   1.000000e+10  163765.64  0.3676682  76153.29
# 
# RMSE was used to select the optimal model using the smallest value.
# The final value used for the model was lambda = 0.01.
```

### (ii) Modelo de Regressão Lasso

```{r eval=FALSE}
fractionGrid <- expand.grid(fraction = seq(.1, .9, length = 100))

lasso <- train(votos ~ ., 
               data = train,
               method = "lasso",
               trControl = fitControl,
               preProcess = c('scale', 'center', 'nzv'),
               tuneGrid = fractionGrid)
lasso
```

```{r}
# The lasso 
# 
# 7476 samples
#   18 predictor
# 
# Pre-processing: scaled (27), centered
#  (27), remove (28) 
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 6728, 6730, 6728, 6728, 6728, 6729, ... 
# Resampling results across tuning parameters:
# 
#   fraction   RMSE      Rsquared   MAE     
#   0.1000000  38642.19  0.3848477  16315.32
#   0.1080808  38834.80  0.3828931  16324.38
#   0.1161616  39032.77  0.3811543  16333.44
#   0.1242424  39235.42  0.3796083  16342.50
#   0.1323232  39442.17  0.3782330  16351.55
#   0.1404040  39652.53  0.3770081  16360.61
#   0.1484848  39866.06  0.3759153  16369.67
#   0.1565657  40082.41  0.3749386  16378.73
#   0.1646465  40301.24  0.3740637  16387.79
#   0.1727273  40522.30  0.3732780  16396.85
#   0.1808081  40745.34  0.3725709  16405.91
#   0.1888889  40970.15  0.3719329  16414.97
#   0.1969697  41196.56  0.3713559  16424.03
#   0.2050505  41424.41  0.3708327  16433.09
#   0.2131313  41653.56  0.3703572  16442.15
#   0.2212121  41883.88  0.3699240  16451.21
#   0.2292929  42115.28  0.3695285  16460.27
#   0.2373737  42347.66  0.3691666  16469.33
#   0.2454545  42580.92  0.3688347  16478.39
#   0.2535354  42815.00  0.3685298  16487.45
#   0.2616162  43049.83  0.3682489  16496.51
#   0.2696970  43285.35  0.3679899  16505.57
#   0.2777778  43521.50  0.3677504  16514.63
#   0.2858586  43758.24  0.3675287  16523.69
#   0.2939394  43995.52  0.3673230  16532.75
#   0.3020202  44233.30  0.3671319  16541.81
#   0.3101010  44471.54  0.3669541  16550.87
#   0.3181818  44710.22  0.3667884  16559.93
#   0.3262626  44949.30  0.3666337  16568.99
#   0.3343434  45188.76  0.3664891  16578.05
#   0.3424242  45428.57  0.3663537  16587.11
#   0.3505051  45668.71  0.3662268  16596.17
#   0.3585859  45909.15  0.3661077  16605.23
#   0.3666667  46149.89  0.3659958  16614.29
#   0.3747475  46390.89  0.3658905  16623.35
#   0.3828283  46632.15  0.3657913  16632.41
#   0.3909091  46873.65  0.3656978  16641.47
#   0.3989899  47115.37  0.3656095  16650.52
#   0.4070707  47357.31  0.3655260  16659.58
#   0.4151515  47599.45  0.3654471  16668.64
#   0.4232323  47841.78  0.3653723  16677.70
#   0.4313131  48084.29  0.3653015  16686.76
#   0.4393939  48326.97  0.3652343  16695.82
#   0.4474747  48569.81  0.3651705  16704.88
#   0.4555556  48812.80  0.3651098  16713.94
#   0.4636364  49055.94  0.3650521  16723.00
#   0.4717172  49299.22  0.3649972  16732.06
#   0.4797980  49542.64  0.3649449  16741.12
#   0.4878788  49786.17  0.3648950  16750.18
#   0.4959596  50029.83  0.3648474  16759.24
#   0.5040404  50273.60  0.3648019  16768.30
#   0.5121212  50517.48  0.3647585  16777.36
#   0.5202020  50761.46  0.3647170  16786.42
#   0.5282828  51005.54  0.3646773  16795.48
#   0.5363636  51249.72  0.3646393  16804.54
#   0.5444444  51493.99  0.3646028  16813.60
#   0.5525253  51738.35  0.3645679  16822.66
#   0.5606061  51982.78  0.3645344  16831.72
#   0.5686869  52227.30  0.3645022  16840.78
#   0.5767677  52471.90  0.3644714  16849.84
#   0.5848485  52716.57  0.3644417  16858.90
#   0.5929293  52961.31  0.3644132  16867.96
#   0.6010101  53206.11  0.3643858  16877.02
#   0.6090909  53450.98  0.3643594  16886.08
#   0.6171717  53695.92  0.3643340  16895.14
#   0.6252525  53940.91  0.3643095  16904.20
#   0.6333333  54185.97  0.3642859  16913.26
#   0.6414141  54431.08  0.3642632  16922.32
#   0.6494949  54676.24  0.3642412  16931.38
#   0.6575758  54921.45  0.3642201  16940.44
#   0.6656566  55166.72  0.3641996  16949.49
#   0.6737374  55412.03  0.3641799  16958.55
#   0.6818182  55657.39  0.3641608  16967.61
#   0.6898990  55902.80  0.3641424  16976.67
#   0.6979798  56148.24  0.3641245  16985.73
#   0.7060606  56393.73  0.3641073  16994.79
#   0.7141414  56639.27  0.3640906  17003.85
#   0.7222222  56884.84  0.3640744  17012.91
#   0.7303030  57130.44  0.3640588  17021.97
#   0.7383838  57376.09  0.3640436  17031.03
#   0.7464646  57621.77  0.3640289  17040.09
#   0.7545455  57867.48  0.3640147  17049.15
#   0.7626263  58113.23  0.3640008  17058.21
#   0.7707071  58359.01  0.3639874  17067.27
#   0.7787879  58604.82  0.3639744  17076.33
#   0.7868687  58850.67  0.3639618  17085.39
#   0.7949495  59096.54  0.3639495  17094.45
#   0.8030303  59342.44  0.3639376  17103.51
#   0.8111111  59588.37  0.3639260  17112.57
#   0.8191919  59834.32  0.3639148  17121.63
#   0.8272727  60080.30  0.3639038  17130.69
#   0.8353535  60326.31  0.3638932  17139.75
#   0.8434343  60572.34  0.3638828  17148.81
#   0.8515152  60818.40  0.3638728  17157.87
#   0.8595960  61064.48  0.3638629  17166.93
#   0.8676768  61310.58  0.3638534  17175.99
#   0.8757576  61556.70  0.3638441  17185.05
#   0.8838384  61802.85  0.3638350  17194.11
#   0.8919192  62049.01  0.3638262  17203.17
#   0.9000000  62295.20  0.3638176  17212.23
# 
# RMSE was used to select the optimal model
#  using the smallest value.
# The final value used for the model was
#  fraction = 0.1.
```

### (iii) Modelo KNN.

```{r eval=FALSE}
knnGrid <- expand.grid(k = seq(1, 100, length=100))

knn <- train(votos ~ .,
             data = train,
             method = "knn",
             trControl = fitControl,
             preProcess = c('scale', 'center', 'nzv'),
             tuneGrid = knnGrid)
knn
```

```{r}
# k-Nearest Neighbors 
# 
# 7476 samples
#   18 predictor
# 
# No pre-processing
# Resampling: Bootstrapped (25 reps) 
# Summary of sample sizes: 7476, 7476, 7476, 7476, 7476, 7476, ... 
# Resampling results across tuning parameters:
# 
#   k    RMSE      Rsquared   MAE     
#     1  45213.68  0.3078302  16625.76
#     2  41545.88  0.3565453  15456.86
#     3  39399.75  0.3908016  14709.42
#     4  37982.61  0.4161435  14217.22
#     5  37064.22  0.4343042  13880.07
#     6  36422.35  0.4469044  13623.35
#     7  35638.78  0.4631769  13392.83
#     8  35264.83  0.4716181  13251.53
#     9  34877.92  0.4803671  13132.82
#    10  34565.45  0.4870318  13000.10
#    11  34335.18  0.4922678  12903.87
#    12  34281.14  0.4931635  12854.01
#    13  34209.67  0.4946393  12799.24
#    14  34122.80  0.4968408  12746.66
#    15  33996.95  0.4998398  12698.98
#    16  33864.56  0.5030152  12640.20
#    17  33793.99  0.5046038  12589.80
#    18  33698.80  0.5069042  12560.97
#    19  33617.40  0.5087935  12530.52
#    20  33542.76  0.5105141  12494.31
#    21  33500.40  0.5115158  12470.55
#    22  33436.84  0.5130813  12443.20
#    23  33389.28  0.5142647  12420.53
#    24  33339.81  0.5155170  12397.72
#    25  33310.88  0.5161730  12377.90
#    26  33290.45  0.5166431  12362.59
#    27  33252.79  0.5175801  12345.35
#    28  33233.85  0.5181147  12335.03
#    29  33211.78  0.5186241  12324.23
#    30  33189.00  0.5192179  12309.75
#    31  33169.69  0.5197532  12298.03
#    32  33154.01  0.5201032  12287.27
#    33  33141.07  0.5205153  12281.34
#    34  33115.87  0.5211692  12274.39
#    35  33099.18  0.5216040  12266.71
#    36  33096.62  0.5215868  12258.36
#    37  33084.34  0.5218737  12249.33
#    38  33064.41  0.5223966  12242.08
#    39  33052.05  0.5227198  12233.52
#    40  33049.29  0.5227625  12228.61
#    41  33050.62  0.5227324  12225.13
#    42  33045.78  0.5228572  12219.25
#    43  33022.03  0.5234416  12210.71
#    44  33010.28  0.5237348  12206.70
#    45  32993.06  0.5241816  12198.17
#    46  32992.31  0.5242105  12197.10
#    47  32987.96  0.5242939  12194.98
#    48  32987.46  0.5242795  12195.03
#    49  32982.97  0.5243754  12194.32
#    50  32989.31  0.5241897  12195.64
#    51  32981.60  0.5243877  12192.93
#    52  32975.29  0.5245455  12190.13
#    53  32966.86  0.5248117  12189.98
#    54  32967.07  0.5247999  12186.23
#    55  32968.49  0.5247470  12185.00
#    56  32963.87  0.5248648  12182.59
#    57  32966.53  0.5247654  12180.78
#    58  32962.07  0.5248807  12175.87
#    59  32953.55  0.5251178  12169.42
#    60  32933.80  0.5256713  12160.77
#    61  32921.91  0.5259926  12156.71
#    62  32920.41  0.5260119  12156.22
#    63  32918.59  0.5260521  12154.61
#    64  32911.43  0.5262210  12151.88
#    65  32903.00  0.5264219  12149.75
#    66  32897.28  0.5265840  12150.07
#    67  32892.02  0.5267242  12145.83
#    68  32888.74  0.5267889  12143.66
#    69  32885.07  0.5268976  12140.80
#    70  32882.54  0.5269493  12137.94
#    71  32881.14  0.5270104  12135.91
#    72  32875.97  0.5271429  12131.10
#    73  32872.56  0.5272448  12126.79
#    74  32869.40  0.5273205  12124.75
#    75  32863.04  0.5275055  12122.81
#    76  32856.44  0.5276933  12117.93
#    77  32851.15  0.5278304  12115.27
#    78  32844.54  0.5280507  12116.12
#    79  32836.58  0.5282672  12112.66
#    80  32826.16  0.5285583  12109.22
#    81  32816.81  0.5288073  12105.11
#    82  32817.46  0.5287867  12106.09
#    83  32815.52  0.5288465  12104.28
#    84  32813.13  0.5289143  12103.35
#    85  32807.03  0.5290886  12098.99
#    86  32805.04  0.5291528  12096.12
#    87  32803.97  0.5291892  12095.53
#    88  32796.89  0.5293873  12093.83
#    89  32795.01  0.5294441  12088.94
#    90  32791.74  0.5295260  12087.38
#    91  32789.39  0.5295797  12084.43
#    92  32783.19  0.5297495  12081.60
#    93  32776.15  0.5299396  12079.39
#    94  32777.86  0.5298982  12081.68
#    95  32769.56  0.5301468  12080.57
#    96  32758.98  0.5304399  12075.80
#    97  32754.42  0.5305696  12074.00
#    98  32754.87  0.5305455  12074.17
#    99  32751.96  0.5306246  12073.10
#   100  32748.21  0.5307155  12071.72
# 
# RMSE was used to select the optimal model using the smallest value.
# The final value used for the model was k = 100.
```

## Compare os três modelos em termos do erro RMSE de validação cruzada.

O modelo com menor valor par ao RMSE foi o KNN, cujo valor final foi 32748.21 para k = 100. O modelo Lasso vem logo em seguida com o segundo menor valor, que foi 38642.19 para fraction = 0.1. Por último, o modelo com um maior valor de RMSE foi o Ridge, que, selecionando lambda = 0.01 resultou em 39363.13.

## Quais as variáveis mais importantes segundo o modelo de regressão Ridge e Lasso?  Variáveis foram descartadas pelo Lasso? Quais?

Para o modelo Ridge, temos:

```{r}
ggplot(varImp(ridge))
```

De acordo com o gráfico, as variáveis mais importantes são, em ordem de importância:
1. `total_receita`
2. `total_despesa`
3. `recursos_de_pessoas_juridicas`
4. `recursos_de_pessoas_fisicas`
5. `quantidade_fornecedores`
6. `quantidade_despesas`
7. `media_receita`
8. `recursos_de_partido_politico`
9. `quantidade_doadores`
10. `quantidade_doacoes`
11. `grau`
12. `estado_civil`
13. `partido`
14. `sexo`

Para o modelo Lasso, temos:

```{r}
ggplot(varImp(lasso))
```

De acordo com o gráfico, vemos que a maioria das variáveis mais importantes para o modelo Ridge são também importantes para o modelo Lasso.
As variáveis descartadas pelo modelo Lasso, são:
* `ano`
* `recursos_de_outros_candidatos.comites`
* `media_despesa`
* `recursos_proprios`

## Re-treine o melhor modelo (usando os melhores valores de parâmetros encontrados em todos os dados, sem usar validação cruzada).

```{r eval=FALSE}
best.grid <- expand.grid(k = knn$bestTune)

best.model <- train(votos ~ .,
                    data = train,
                    method = "knn",
                    tuneGrid = best.grid)
best.model
```

## Use esse último modelo treinado para prever os dados de teste disponíveis no challenge disponível plataforma Kaggle.

```{r}
test <- read.csv(here("data/test.csv"))

submission <- test %>%
  select(sequencial_candidato)

test <- test %>% 
  select(-sequencial_candidato,
         -nome,
         -cargo,
         -uf,
         -ocupacao)

predictions <- predict(best.model, test)
submission$votos <- predictions
submission <- submission %>% 
  select(ID = sequencial_candidato,
         votos = votos)
write.csv(x = submission,
          file = "sample_submission.csv",
          row.names = FALSE)
```

