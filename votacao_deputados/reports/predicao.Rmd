---
title: "Predição de Votação de Deputados"
author: "Valter Lucena"
date: "29 de outubro de 2018"
output: 
  html_document:
    df_print: paged
    toc: yes
    toc_float: yes
  html_notebook:
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(here)
library(caret)
options(max.print = .Machine$integer.max)
set.seed(37)
theme_set(theme_minimal())
```

# Introdução

Nesta análise iremos construir modelos preditivos de regressão para a predição de votação de candidatos à Câmara Federal de Deputados. Anteriormente, [nesta análise](http://rpubs.com/valterlucena/votacao_regressao), buscamos explicar essas votações através da regressão linear multivariada. Utilizaremos, agora, as seguintes técnicas:

* **Regressão Ridge**: todo
* **Regressão Lasso**: todo
* **Regressão KNN**: todo

```{r message=FALSE, warning=FALSE}
train <- read_csv(here("data/train.csv"), progress = FALSE)
test <- read_csv(here("data/test.csv"), progress = FALSE)
```

# Modelos

```{r}
train %>% names()
```


```{r}
train <- train %>% 
  select(-sequencial_candidato,
         -nome,
         -cargo,
         -uf,
         -ocupacao)

fitControl <- trainControl(method = "cv", # boot", "boot632", "cv", "repeatedcv", "LOOCV", "LGOCV"
                           number = 10,
                           search = "random")
```

```{r}
lambdaGrid <- expand.grid(lambda = 10^seq(10, -2, length=100))

ridge <- train(votos ~ ., 
               data = train,
               method = "ridge",
               trControl = fitControl,
               preProcess = c('scale', 'center', 'nzv'),
               tuneGrid = lambdaGrid)
ridge

# Ridge Regression 
# 
# 7476 samples
#   18 predictor
# 
# Pre-processing: scaled (27), centered (27), remove (28) 
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 6729, 6728, 6728, 6729, 6728, 6728, ... 
# Resampling results across tuning parameters:
# 
#   lambda        RMSE       Rsquared   MAE     
#   1.000000e-02   37711.85  0.4061784  16244.12
#   1.321941e-02   37745.16  0.4057783  16243.99
#   1.747528e-02   37786.39  0.4052913  16241.94
#   2.310130e-02   37835.45  0.4047466  16239.29
#   3.053856e-02   37892.75  0.4041738  16232.08
#   4.037017e-02   37959.67  0.4035965  16218.49
#   5.336699e-02   38039.11  0.4030264  16195.75
#   7.054802e-02   38136.10  0.4024579  16163.88
#   9.326033e-02   38258.76  0.4018662  16122.54
#   1.232847e-01   38419.64  0.4012067  16076.94
#   1.629751e-01   38637.82  0.4004180  16032.34
#   2.154435e-01   38942.34  0.3994297  15994.08
#   2.848036e-01   39377.26  0.3981734  15977.47
#   3.764936e-01   40008.83  0.3965979  16017.98
#   4.977024e-01   40934.10  0.3946823  16162.08
#   6.579332e-01   42288.87  0.3924455  16485.10
#   8.697490e-01   44249.95  0.3899469  17083.18
#   1.149757e+00   47024.57  0.3872803  18110.06
#   1.519911e+00   50821.09  0.3845595  19694.70
#   2.009233e+00   55802.48  0.3819012  21995.99
#   2.656088e+00   62035.04  0.3794076  25031.07
#   3.511192e+00   69450.32  0.3771543  28867.50
#   4.641589e+00   77834.80  0.3751844  33283.88
#   6.135907e+00   86852.20  0.3735110  38045.51
#   8.111308e+00   96093.73  0.3721232  42937.51
#   1.072267e+01  105143.43  0.3709947  47732.85
#   1.417474e+01  113640.29  0.3700917  52246.69
#   1.873817e+01  121321.16  0.3693781  56335.60
#   2.477076e+01  128036.62  0.3688199  59914.17
#   3.274549e+01  133742.34  0.3683865  62952.37
#   4.328761e+01  138475.08  0.3680522  65468.88
#   5.722368e+01  142323.97  0.3677955  67514.43
#   7.564633e+01  145404.47  0.3675990  69151.53
#   1.000000e+02  147838.78  0.3674491  70446.20
#   1.321941e+02  149743.25  0.3673349  71459.36
#   1.747528e+02  151221.57  0.3672481  72246.56
#   2.310130e+02  152362.15  0.3671822  72853.98
#   3.053856e+02  153238.03  0.3671322  73321.14
#   4.037017e+02  153908.23  0.3670943  73678.59
#   5.336699e+02  154419.65  0.3670655  73951.32
#   7.054802e+02  154809.08  0.3670438  74158.96
#   9.326033e+02  155105.15  0.3670273  74316.80
#   1.232847e+03  155329.97  0.3670148  74436.65
#   1.629751e+03  155500.53  0.3670054  74527.56
#   2.154435e+03  155629.84  0.3669983  74596.48
#   2.848036e+03  155727.81  0.3669929  74648.69
#   3.764936e+03  155802.02  0.3669888  74688.24
#   4.977024e+03  155858.21  0.3669857  74718.18
#   6.579332e+03  155900.74  0.3669833  74740.85
#   8.697490e+03  155932.93  0.3669816  74758.01
#   1.149757e+04  155957.30  0.3669802  74770.99
#   1.519911e+04  155975.73  0.3669792  74780.82
#   2.009233e+04  155989.68  0.3669784  74788.25
#   2.656088e+04  156000.24  0.3669779  74793.87
#   3.511192e+04  156008.22  0.3669774  74798.13
#   4.641589e+04  156014.26  0.3669771  74801.35
#   6.135907e+04  156018.83  0.3669768  74803.78
#   8.111308e+04  156022.29  0.3669766  74805.62
#   1.072267e+05  156024.90  0.3669765  74807.02
#   1.417474e+05  156026.88  0.3669764  74808.07
#   1.873817e+05  156028.38  0.3669763  74808.87
#   2.477076e+05  156029.51  0.3669762  74809.47
#   3.274549e+05  156030.37  0.3669762  74809.93
#   4.328761e+05  156031.01  0.3669762  74810.27
#   5.722368e+05  156031.50  0.3669761  74810.54
#   7.564633e+05  156031.88  0.3669761  74810.73
#   1.000000e+06  156032.16  0.3669761  74810.88
#   1.321941e+06  156032.37  0.3669761  74811.00
#   1.747528e+06  156032.53  0.3669761  74811.08
#   2.310130e+06  156032.65  0.3669761  74811.15
#   3.053856e+06  156032.74  0.3669761  74811.19
#   4.037017e+06  156032.81  0.3669761  74811.23
#   5.336699e+06  156032.86  0.3669761  74811.26
#   7.054802e+06  156032.90  0.3669761  74811.28
#   9.326033e+06  156032.93  0.3669761  74811.30
#   1.232847e+07  156032.96  0.3669761  74811.31
#   1.629751e+07  156032.97  0.3669761  74811.32
#   2.154435e+07  156032.99  0.3669761  74811.32
#   2.848036e+07  156033.00  0.3669761  74811.33
#   3.764936e+07  156033.00  0.3669761  74811.33
#   4.977024e+07  156033.01  0.3669761  74811.34
#   6.579332e+07  156033.01  0.3669761  74811.34
#   8.697490e+07  156033.02  0.3669761  74811.34
#   1.149757e+08  156033.02  0.3669761  74811.34
#   1.519911e+08  156033.02  0.3669761  74811.34
#   2.009233e+08  156033.02  0.3669761  74811.34
#   2.656088e+08  156033.02  0.3669761  74811.34
#   3.511192e+08  156033.02  0.3669761  74811.35
#   4.641589e+08  156033.03  0.3669761  74811.35
#   6.135907e+08  156033.03  0.3669761  74811.35
#   8.111308e+08  156033.03  0.3669761  74811.35
#   1.072267e+09  156033.03  0.3669761  74811.35
#   1.417474e+09  156033.03  0.3669761  74811.35
#   1.873817e+09  156033.03  0.3669761  74811.35
#   2.477076e+09  156033.03  0.3669761  74811.35
#   3.274549e+09  156033.03  0.3669761  74811.35
#   4.328761e+09  156033.03  0.3669761  74811.35
#   5.722368e+09  156033.03  0.3669761  74811.35
#   7.564633e+09  156033.03  0.3669761  74811.35
#   1.000000e+10  156033.03  0.3669761  74811.35
# 
# RMSE was used to select the optimal model using the smallest value.
# The final value used for the model was lambda = 0.01.

```

```{r}
fractionGrid <- expand.grid(fraction = seq(.1, .9, length = 100))

lasso <- train(votos ~ ., 
               data = train,
               method = "lasso",
               trControl = fitControl,
               preProcess = c('scale', 'center', 'nzv'),
               tuneGrid = fractionGrid)
lasso

# The lasso 
# 
# 7476 samples
#   18 predictor
# 
# Pre-processing: scaled (27), centered (27), remove (28) 
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 6728, 6728, 6729, 6729, 6729, 6728, ... 
# Resampling results across tuning parameters:
# 
#   fraction   RMSE      Rsquared   MAE     
#   0.1000000  38330.01  0.4023565  16986.15
#   0.1080808  38288.59  0.4023563  16949.45
#   0.1161616  38249.50  0.4023561  16915.91
#   0.1242424  38212.82  0.4023559  16883.14
#   0.1323232  38178.64  0.4023557  16850.46
#   0.1404040  38147.03  0.4023555  16817.90
#   0.1484848  38118.15  0.4023707  16785.52
#   0.1565657  38092.38  0.4025041  16755.66
#   0.1646465  38068.97  0.4026106  16726.46
#   0.1727273  38047.97  0.4026952  16698.46
#   0.1808081  38029.45  0.4027622  16670.56
#   0.1888889  38011.79  0.4030977  16644.26
#   0.1969697  37995.61  0.4034003  16618.09
#   0.2050505  37980.98  0.4036637  16593.33
#   0.2131313  37967.93  0.4038923  16570.07
#   0.2212121  37956.46  0.4040901  16547.35
#   0.2292929  37946.60  0.4042604  16525.66
#   0.2373737  37938.37  0.4044062  16504.36
#   0.2454545  37936.91  0.4043277  16490.38
#   0.2535354  37936.00  0.4043003  16479.67
#   0.2616162  37934.36  0.4043368  16468.47
#   0.2696970  37933.86  0.4043487  16457.86
#   0.2777778  37934.49  0.4043378  16447.68
#   0.2858586  37935.52  0.4043298  16437.60
#   0.2939394  37936.20  0.4043448  16427.75
#   0.3020202  37937.37  0.4043517  16418.02
#   0.3101010  37939.31  0.4043441  16408.62
#   0.3181818  37941.15  0.4043445  16400.54
#   0.3262626  37941.19  0.4044033  16393.50
#   0.3343434  37940.20  0.4045017  16386.13
#   0.3424242  37939.13  0.4045893  16380.65
#   0.3505051  37938.06  0.4046695  16376.06
#   0.3585859  37936.97  0.4047521  16371.41
#   0.3666667  37935.35  0.4048499  16366.64
#   0.3747475  37933.90  0.4049414  16361.89
#   0.3828283  37932.61  0.4050286  16357.16
#   0.3909091  37931.45  0.4051127  16352.44
#   0.3989899  37930.43  0.4051938  16347.73
#   0.4070707  37929.54  0.4052717  16343.01
#   0.4151515  37928.80  0.4053468  16338.39
#   0.4232323  37928.19  0.4054188  16333.77
#   0.4313131  37927.72  0.4054880  16329.16
#   0.4393939  37927.39  0.4055544  16324.59
#   0.4474747  37927.20  0.4056181  16320.07
#   0.4555556  37927.15  0.4056790  16315.59
#   0.4636364  37927.23  0.4057372  16311.24
#   0.4717172  37927.46  0.4057928  16307.05
#   0.4797980  37927.57  0.4058525  16303.18
#   0.4878788  37927.69  0.4059130  16299.30
#   0.4959596  37927.94  0.4059710  16295.44
#   0.5040404  37928.33  0.4060266  16291.59
#   0.5121212  37928.74  0.4060709  16288.41
#   0.5202020  37929.10  0.4061020  16285.97
#   0.5282828  37929.53  0.4061317  16283.54
#   0.5363636  37930.03  0.4061599  16281.35
#   0.5444444  37930.67  0.4061856  16279.38
#   0.5525253  37931.59  0.4062068  16277.45
#   0.5606061  37932.54  0.4062278  16275.56
#   0.5686869  37933.56  0.4062476  16273.70
#   0.5767677  37934.28  0.4062749  16271.84
#   0.5848485  37935.08  0.4063011  16270.03
#   0.5929293  37935.94  0.4063259  16268.31
#   0.6010101  37936.87  0.4063496  16266.63
#   0.6090909  37937.71  0.4063726  16265.06
#   0.6171717  37938.69  0.4063916  16263.62
#   0.6252525  37939.74  0.4064094  16262.19
#   0.6333333  37940.84  0.4064262  16260.76
#   0.6414141  37942.01  0.4064418  16259.41
#   0.6494949  37943.24  0.4064564  16258.18
#   0.6575758  37944.53  0.4064699  16257.05
#   0.6656566  37948.50  0.4064349  16255.95
#   0.6737374  37952.61  0.4063965  16255.01
#   0.6818182  37956.02  0.4063636  16254.48
#   0.6898990  37956.27  0.4063823  16253.99
#   0.6979798  37955.94  0.4064096  16253.53
#   0.7060606  37955.59  0.4064278  16253.23
#   0.7141414  37955.23  0.4064415  16253.01
#   0.7222222  37954.88  0.4064552  16252.80
#   0.7303030  37954.53  0.4064687  16252.60
#   0.7383838  37954.18  0.4064821  16252.40
#   0.7464646  37953.85  0.4064954  16252.21
#   0.7545455  37953.45  0.4065099  16251.98
#   0.7626263  37953.39  0.4065136  16251.93
#   0.7707071  37953.37  0.4065159  16251.91
#   0.7787879  37953.36  0.4065181  16251.88
#   0.7868687  37953.34  0.4065204  16251.86
#   0.7949495  37953.33  0.4065226  16251.84
#   0.8030303  37953.32  0.4065248  16251.82
#   0.8111111  37953.31  0.4065269  16251.80
#   0.8191919  37953.31  0.4065290  16251.78
#   0.8272727  37953.30  0.4065311  16251.77
#   0.8353535  37953.30  0.4065331  16251.75
#   0.8434343  37953.32  0.4065349  16251.75
#   0.8515152  37953.35  0.4065366  16251.76
#   0.8595960  37953.38  0.4065383  16251.76
#   0.8676768  37953.41  0.4065400  16251.77
#   0.8757576  37953.44  0.4065417  16251.77
#   0.8838384  37953.47  0.4065433  16251.78
#   0.8919192  37953.50  0.4065449  16251.78
#   0.9000000  37953.54  0.4065464  16251.79
# 
# RMSE was used to select the optimal model using the smallest value.
# The final value used for the model was fraction = 0.4555556.
```

```{r}
knn <- train(votos ~ .,
             data = train,
             method = "knn",
             trControl = fitControl,
             preProcess = c('scale', 'center', 'nzv'),
             tuneLenght = 20)
plot(knn)

# k-Nearest Neighbors 
# 
# 7476 samples
#   18 predictor
# 
# Pre-processing: scaled (27), centered (27), remove (28) 
# Resampling: Cross-Validated (5 fold, repeated 5 times) 
# Summary of sample sizes: 5980, 5980, 5983, 5980, 5981, 5981, ... 
# Resampling results across tuning parameters:
# 
#   k  RMSE      Rsquared   MAE     
#   5  34731.23  0.4773493  12915.87
#   7  34679.56  0.4785218  12906.56
#   9  34479.83  0.4852873  12867.67
# 
# RMSE was used to select the optimal model using the smallest value.
# The final value used for the model was k = 9.

```

# 